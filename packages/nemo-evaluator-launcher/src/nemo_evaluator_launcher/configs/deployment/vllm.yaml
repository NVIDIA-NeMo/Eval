type: vllm
image: vllm/vllm-openai:latest
checkpoint_path: ???
served_model_name: ???
port: 8000
tensor_parallel_size: 8
pipeline_parallel_size: 1
data_parallel_size: 1
extra_args: ""
env_vars: {} # {name: value} dict

endpoints:
  chat: /v1/chat/completions
  completions: /v1/completions
  health: /health

command: vllm serve /checkpoint
  --tensor-parallel-size=${deployment.tensor_parallel_size}
  --pipeline-parallel-size=${deployment.pipeline_parallel_size}
  --data-parallel-size=${deployment.data_parallel_size}
  --port ${deployment.port}
  --trust-remote-code
  --served-model-name ${deployment.served_model_name}
  --enforce-eager
  --gpu-memory-utilization 0.95
  ${deployment.extra_args}
