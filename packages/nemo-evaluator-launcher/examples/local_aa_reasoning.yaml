# Run this config with `nv-eval run --config-dir <YOUR_CONFIGS_DIR> --config-name <YOUR_CONFIG_FILENAME>`.

defaults:
  - execution: local
  - deployment: none
  - _self_

execution:
  output_dir: ??? # Set the output directory for your experiments; each `nv-eval run` invocation will create a subdirectory

  # OPTIONAL: export results to mlflow; comment out if not needed
  # install mlflow with `pip install nemo-evaluator-launcher-internal[mlflow] --index-url https://gitlab-master.nvidia.com/api/v4/projects/155749/packages/pypi/simple`
  auto_export:
    destinations: ["mlflow"]
    configs:
      mlflow:
        tracking_uri: http://10.64.49.50:5003
        experiment_name: seminalysis_${target.api_endpoint.model_id}
        description: "Evaluation for semianalysis"
        tags:
          framework: "vLLM"
          precision: "bf16"
          tags: "test"

target:
  api_endpoint:
    # model_id: Qwen/Qwen3-8B
    # url: https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/2ab8a943-43ae-4867-8bd0-dea41e5097f7
    model_id: deepseek-ai/deepseek-r1
    url: https://integrate.api.nvidia.com/v1/chat/completions
    api_key_name: NGC_API_KEY

evaluation:
  overrides: # these overrides apply to all tasks; for task-specific overrides, use the `overrides` field
    config.params.parallelism: 4 # Number of concurrent requests per each benchmark
    config.params.request_timeout: 3600
    config.params.max_retries: 10
    config.params.max_new_tokens: 65536
    config.params.temperature: 0.6
    config.params.top_p: 0.95
    target.api_endpoint.adapter_config.use_nvcf: true
    target.api_endpoint.adapter_config.use_reasoning: true # strips off reasoning part before evaluating a response
    # Uncomment below overrides if using Llama Nemotron family of models - turns on thinking
    # target.api_endpoint.adapter_config.use_system_prompt: true
    # target.api_endpoint.adapter_config.custom_system_prompt: >-
    #   "detailed thinking on"
  tasks:
    - name: livecodebench_0724_0125
      overrides: # task-specific overrides
        config.params.extra.num_process_evaluate: 10 # number of processes for the code evaluation phase; memory heavy

    - name: aa_scicode
      overrides:
        target.api_endpoint.adapter_config.include_if_reasoning_not_finished: false # if reasoning hasn't finished, returns empty string; otherwise, overflows max context length

    - name: AA_math_test_500
      env_vars:
        JUDGE_API_KEY: JUDGE_API_KEY_AA_MATH_TEST_500 # An api key for https://build.nvidia.com/meta/llama-3_3-70b-instruct

    - name: AA_AIME_2024
      env_vars:
        JUDGE_API_KEY: JUDGE_API_KEY_FOR_AA_AIME_2024 # An api key for https://build.nvidia.com/meta/llama-3_3-70b-instruct

    - name: mmlu_pro

    - name: hle
      env_vars:
        HF_TOKEN: HF_TOKEN_FOR_HLE # Click request access for HLE: https://huggingface.co/datasets/cais/hle
        OPENAI_TOKEN_URL: OPENAI_TOKEN_URL_FOR_HLE
        OPENAI_CLIENT_ID: OPENAI_CLIENT_ID_FOR_HLE # the key must have access to gpt-4o
        OPENAI_CLIENT_SECRET: OPENAI_CLIENT_SECRET_FOR_HLE # the key must have access to gpt-4o
        OPENAI_SCOPE: OPENAI_SCOPE_FOR_HLE

    - name: gpqa_diamond_aa_v2
      env_vars:
        HF_TOKEN: HF_TOKEN_FOR_GPQA_DIAMOND # Click request access for GPQA-Diamond: https://huggingface.co/datasets/Idavidrein/gpqa
