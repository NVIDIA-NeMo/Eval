# Caching

The caching interceptor stores and retrieves responses to improve performance, reduce API costs, and enable reproducible evaluations.

## Overview

The `CachingInterceptor` implements a sophisticated caching system that can store responses based on request content, enabling faster re-runs of evaluations and reducing costs when using paid APIs.

## Configuration

### AdapterConfig Parameters

```python
from nemo_evaluator.adapters.adapter_config import AdapterConfig

adapter_config = AdapterConfig(
    # Enable caching
    use_caching=True,
    
    # Cache configuration
    caching_dir="./evaluation_cache",
    reuse_cached_responses=True,
    
    # Storage options
    save_responses=True,
    
    # Cache key configuration
    cache_key_fields=["messages", "temperature", "max_new_tokens"]
)
```

### CLI Configuration
```bash
--overrides 'target.api_endpoint.adapter_config.use_caching=True,target.api_endpoint.adapter_config.caching_dir=./cache,target.api_endpoint.adapter_config.reuse_cached_responses=True'
```

### YAML Configuration
```yaml
target:
  api_endpoint:
    adapter_config:
      use_caching: true
      caching_dir: "./evaluation_cache"
      reuse_cached_responses: true
      save_responses: true
```

## Configuration Options

| Parameter | Description | Default | Type |
|-----------|-------------|---------|------|
| `use_caching` | Enable response caching | `False` | bool |
| `caching_dir` | Directory to store cache files | `"./cache"` | str |
| `reuse_cached_responses` | Use cached responses when available | `True` | bool |
| `save_responses` | Save responses to cache storage | `True` | bool |
| `cache_key_fields` | Fields to include in cache key generation | `["messages"]` | list |

## Cache Key Generation

The cache key is generated by hashing the relevant request fields:

```python
# Example cache key generation
cache_key = hash({
    "messages": [{"role": "user", "content": "What is 2+2?"}],
    "temperature": 0.0,
    "max_new_tokens": 512,
    "model_id": "llama-3.1-8b"
})
```

## Cache Storage Format

### Cache Entry Structure
```json
{
    "cache_key": "abc123def456",
    "request": {
        "messages": [{"role": "user", "content": "What is 2+2?"}],
        "temperature": 0.0,
        "max_new_tokens": 512
    },
    "response": {
        "choices": [
            {
                "message": {
                    "role": "assistant", 
                    "content": "2+2 equals 4."
                }
            }
        ],
        "usage": {"prompt_tokens": 45, "completion_tokens": 8}
    },
    "timestamp": "2025-01-08T10:30:00Z",
    "model_id": "megatron_model",
    "status_code": 200
}
```

## Cache Behavior

### Cache Hit Process
1. **Request arrives** at the caching interceptor
2. **Generate cache key** from request parameters  
3. **Check cache** for existing response
4. **Return cached response** if found (sets `cache_hit=True`)
5. **Skip API call** and continue to next interceptor

### Cache Miss Process  
1. **Request continues** to endpoint interceptor
2. **Response received** from model API
3. **Store response** in cache with generated key
4. **Continue processing** with response interceptors

## Use Cases

- **Development Iterations**: Cache responses during prompt engineering and testing
- **Cost Optimization**: Reduce API costs for repeated evaluations
- **Reproducible Results**: Ensure identical responses for identical requests
- **Performance**: Speed up re-runs of large evaluations
- **Offline Analysis**: Enable offline analysis of cached evaluation data

## Best Practices

### Cache Management
```python
# Configure cache retention
adapter_config = AdapterConfig(
    use_caching=True,
    caching_dir="./cache",
    # Clean up old cache entries
    cache_retention_days=7
)
```

### Selective Caching
```python
# Cache only specific parameters
adapter_config = AdapterConfig(
    use_caching=True,
    cache_key_fields=["messages", "temperature"],  # Exclude max_new_tokens
    reuse_cached_responses=True
)
```
